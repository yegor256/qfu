% The MIT License (MIT)
%
% Copyright (c) 2020 Yegor Bugayenko
%
% Permission is hereby granted, free of charge, to any person obtaining a copy
% of this software and associated documentation files (the "Software"), to deal
% in the Software without restriction, including without limitation the rights
% to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
% copies of the Software, and to permit persons to whom the Software is
% furnished to do so, subject to the following conditions:
%
% The above copyright notice and this permission notice shall be included
% in all copies or substantial portions of the Software.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
% AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
% OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
% SOFTWARE.
\documentclass[12pt,oneside]{article}
\title{A Multi-Factor Method of Measuring the Quality of a Software Project}
\author{Yegor Bugayenko, Ilya Moskvitin}

\usepackage[utf8]{inputenc}
\usepackage{natbib}
  \setlength\bibsep{0pt}
  \citeindextrue
  \setcitestyle{square,numbers}
  \bibliographystyle{abbrvnat}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,allcolors=blue!60!black}
\usepackage{amsmath}
\usepackage{enumitem}
\setlist{nosep}
\begin{document}
\raggedbottom
\setlength{\topskip}{6pt}
\setlength{\parindent}{0pt} % indent first line
\setlength{\parskip}{6pt} % before par

\maketitle

\begin{abstract}
Quality is a complex attribute of a software product. It consists of
multiple ingredients, including the absence of functionality defects,
high performance, data consistency, time to recover after failure, and
many others. Maintainability is one them, which has recently become
more important than it was before, mostly because the cost of
human resources is getting higher every year, while the cost of
computational resources decreases, as was explained by~\citet{yb-hacking}.
Because of that, new methods are required to analyze software projects, evaluate their
maintainability, and give programmers effective recommendations on
how maintainability issues can be fixed. A method of this kind is proposed
below.
\end{abstract}

\section{Introduction}

The suggested method may be used to analyze the quality
of a software project, taking into account a few important
measurable factors, such as consistency and effectiveness of
used programming practices, soundness and solidness of the
architecture, quality of documentation, and so on.

The method has been tested on a number of projects, both
open source and commercial. It was first proposed by~\citet{yb-sins}
as a method of measuring the quality of open source projects developed
by freelancers. Since then, over 200 GitHub repositories were
analyzed by over 40 reviewers and their analysis results were published
by~\citet{yb-award}.

The paper is organized as follows.
Section~\ref{sec:related} covers related work in the area of quality analysis
of software projects.
Section~\ref{sec:method} explains the method, giving instructions
to project reviewers.
Finally, Section~\ref{sec:discussion} identifies validity threats and suggests
alternatives to the metrics explained in the Section~\ref{sec:method}.

\section{Related Work}
\label{sec:related}

There are many software experts who believe that quality of code is the
most important success factor of a software project.
For example,~\citet{martin2009clean} believes that ``it is not enough for code to work,''
it must also be easy to maintain and extend.
\citet{mcconnell2004code} says that ``in the vast majority of systems, efficiency isn't critical,''
meaning that maintainability and other non-performance and non-functional
qualities very often are much more critical.

\section{Method}
\label{sec:method}

The method combines automated and manual techniques. There are $N$
\emph{metrics} to be observed and collected on a software project, which are described
below. Each metric is an integer between zero and ten, where
zero means the lowest quality, while ten means the highest one:

\begin{eqnarray}
M_{1..N} \in [ 0 .. 10 ]
\end{eqnarray}

Each metric has its own \emph{weight} $W_{1..N}$.
The final \emph{score} is calculated
as a weighed arithmetic summary of all measurements $M_i$:

\begin{align}
S = \frac{\sum_i^N M_i \times W_i}{\sum_i^N W_i} \in [0..10]
\end{align}

A group of at least two independent software developers is asked to review the code
repository and provide their scores as $S_1$ and $S_2$. The average between
them is the final score of the project.

The following metrics are used in the method:

\subsection{M1: Programming Practices (W=1.0)}

This metric is one of the most important for the code quality and error-proneness.
It is supposed to spot mistakes in exception handling, resource allocation, memory overflow control,
naming conventions, class and method length control, code complexity,
validity of call chains, validation of input method parameters,
declaration of variables and constants,
and so on. This is an incomplete lists of checks to do:

\begin{itemize}
\item There are no empty exception handlers~\citep[pp.205--208]{eo1};
\item Exceptions are not swallowed~\citep[pp.212]{eo1};
\item There are no redundant exception catchers~\citep[pp.202]{eo1};
\item Objects are thread-safe, where necessary~\citep[pp.89-93]{eo1};
\item Language naming conventions are obeyed;
\item Inbound method arguments are validated for NULL~\citep[pp.146-153]{eo1};
\item The usage of NULL references is minimized~\citep[pp.87--89]{eo1};
\item There are no redundant empty lines~\citep{yb-empty-lines};
\item There are no busy waitings~\citep{so-busywait}.
\end{itemize}

\subsection{M2: Architecture and Design (W=1.0)}

The metric evaluates how sound are the key technical decisions made
in the project. It is assumed that the best technical decision is the
one that leads to the most maintainable software, which will be easier
to modify, read, and support in the future. This is an incomplete lists of
checks to do:

\begin{itemize}
\item Design patterns are used correctly~\citep{gamma1993design,holub2004holub,freeman2008head,fowler2003patterns};
\item Anti-patterns are absent~\citep{jaafar2013mining};
\item Classes are short and their cohesion is high~\citep[pp.93--95]{eo1};
\item Frameworks are used where possible, instead of custom code;
\item Where possible, the latest versions of frameworks are used;
\item Procedural anti-patterns are avoided in OOP code~\citep[pp.19--26]{eo1};
\item Aspect programming is used properly~\citep[pp.210--212]{eo1};
\item Annotations are used when needed~\citep[pp.163]{eo2};
\item Explicit class casting is avoided~\citep[pp.181--186]{eo2};
\item The usage of static methods is minimized~\citep[pp.117--145]{eo2};
\item The usage of getters and setters is minimal~\citep[pp.165--174]{eo1};
\item Objects are immutable where possible~\citep[pp.78--89]{eo2};
\item There are no global variables~\citep{yb-global-vars};
\item DTO usage is minimized~\citep[p.179]{eo2};
\item Utility classes are not used~\citep[p.40]{eo2};
\item No cyclic dependencies;
\item Proper project folders and modules structure.
\end{itemize}

\subsection{M3: Test Coverage (W=0.8)}

There are many different tool on the market to create unit and integration tests,
estimate test code coverage, and improve the quality of code using
the information provided by test coverage collectors. This metric judges
the quality of tests and the existence of test coverage analysis. A reviewer
must pay attention to the following practices:

\begin{itemize}
  \item Unit and integration tests are written;
  \item Low test coverage breaks the build;
  \item Test coverage is published;
  \item Test anti-patterns are absent~\citep{yb-test-anti};
  \item Mutation coverage is controlled~\citep{andrews2006using}.
\end{itemize}

\subsection{M4: Static Analysis (W=0.8)}

The metric validates the existence of static analysis in the project
and its mandatory usage as part of the build pipeline.
There are many well-known tools on the market, both commercial and
open source, which can be used by the project. These tools are recommended:

\begin{itemize}
  \item Java: Checkstyle~\citep{checkstyle}, PMD, SpotBugs
  \item C++: cpplint, cppcheck~\citep{li2019cppcheck}
  \item Ruby: Rubocop~\citep{rubocop}
  \item Go: vet, golang/lint~\citep{golint}, staticcheck, errcheck
  \item JavaScript: ESLint, JSHint, standardJS~\citep{kavaler2019tools}
  \item Python: flake8, PyLint~\citep{squamia2019tools}
\end{itemize}

The metric doesn't judge the quality of the tools, but pays attention to how strict is the
confirmation of the tools being used. For example, a team may use Checkstyle
to validate their Java code, but configured to use only a few checks
out of a few hundred shipped by the creators of Checkstyle.

\begin{itemize}
  \item Code duplications are avoided~\citep[pp.39--40]{eo2};
  \item Checkstyle tools are used;
  \item Only determined imports are used;
  \item Return values of non-void methods are always used;
\end{itemize}

\subsection{M5: Discipline (W=0.6)}

This metric evaluates the process of the software
development. It discovers problems much more related to the team than
to the product under development. It also demonstrates how strongly the project is
protected against human mistakes. Most software development teams use
Version Control Systems (VCS)~\citep{spinellis2005vcs, loeliger2012git} and Continuous Integration/Delivery (CI/CD) systems~\citep{mf-ci, miller2008ci}, but not all software engineers properly understand their functionality. For example, this is what
a reviewer must pay attention to:

\begin{itemize}
  \item CI/CD pipelines are automated;
  \item Release procedure is automated;
  \item Releases are frequent and fully documented;
  \item Git commits have detailed comments~\citep[pp.65--66]{loeliger2012git};
  \item Each code change is traceable to a bug report or a task;
  \item All changes get into the trunk after manual reviews;
  \item The trunk (e.g. Git ``master'' branch) is read-only~\citep[pp.65--66]{pidoux2014git};
  \item Forced ``push'' is not used and code is never deleted;
\end{itemize}

The metric doesn't judge the choice of the tools (sometimes this decision
is not made by the team itself), but only validates the correctness
of their usage.

\subsection{M6: Documentation (W=0.5)}

There are different types of documentation a software project may have,
starting from inline code comments to developers guidelines and user manuals.
The existing of the documentation may be checked with automated tools,
but only a human expert can understand whether the documentation is truly
informative and useful. The following tools are recommended to use:

\begin{itemize}
  \item Repository contains information about project and it's usage;
  \item Code documented according to modern principals and standards;
  \item Special tools and frameworks are used to process documentation.
  \item Java: Javadoc, Swagger
  \item C++: Doxygen, Sphinx
  \item Ruby: Rdoc
  \item Python: Pydoc
\end{itemize}

\section{Discussion}
\label{sec:discussion}

Next step is to apply suggested method to production code and search correlation between final score and concrete metrics. The goal is to find metrics which are most important for the total quality. It might help teams to improve code and development process.

\raggedright
\bibliography{main}

\end{document}
